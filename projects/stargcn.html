<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Dark Matter GNN | Mi-Ru Youn</title>
  <link rel="stylesheet" href="../style.css">
</head>
<body>
  <!-- Site Header -->
  <header class="header-flex">
    <img src="../assets/profile.png" alt="Portrait of Mi-Ru Youn" class="profile-pic">
    <div>
      <h1>Mi-Ru Youn</h1>
      <p>Data Science Master's Student at Boston University</p>
      <nav>
        <a href="../about.html">About</a>
        <a href="../index.html#projects" class="active">Projects</a>
        <a href="../index.html#skills">Skills</a>
        <a href="../index.html#contact">Contact</a>
      </nav>
    </div>
  </header>

  <main>
    <!-- Overview -->
    <section>
      <h2>Overview</h2>
      <p>
        <b>Reproducing GNN-based Inference of Dark Matter Density Profiles</b> explores 
        how <b>graph neural networks (GNNs)</b> can infer the dark matter density 
        profiles of dwarf galaxies from stellar kinematic data. We reproduced 
        the pipeline introduced by Nguyen et al. (2022), validating their 
        results and investigating reproducibility of astrophysical parameter 
        inference using both a scratch-built and library-based GNN.
      </p>
    </section>

    <!-- Problem -->
    <section>
      <h2>Problem</h2>
      <p>
        Traditional astrophysical methods like Jeans modeling rely on 
        assumptions such as dynamical equilibrium and isotropy, which can 
        introduce degeneracies and limit accuracy. This makes it challenging 
        to precisely recover dark matter parameters from stellar phase-space data. 
        Recent advances in <b>graph neural networks</b> provide an opportunity 
        to extract higher-order correlations by modeling stars as graph nodes 
        and their kinematic relationships as edges.
      </p>
    </section>

    <!-- Solution -->
    <section>
      <h2>Solution</h2>
      <ul>
        <li>Implemented a <b>scratch-built GCN</b> in PyTorch with two GCNConv layers.</li>
        <li>Used <b>PyTorch Geometric</b> to build a GNN + normalizing flow pipeline for posterior inference.</li>
        <li>Benchmarked results against a <b>spherical Jeans solver</b> for classical comparison.</li>
        <li>Generated training, validation, and test datasets using <b>StarSampler</b>.</li>
      </ul>
    </section>

    <!-- Implementation -->
    <section>
      <h2>Implementation</h2>
      <p>
        We trained on <b>80,000 simulated galaxies</b> and validated on 10,000, 
        with an additional 10,000 for testing. Optimization used 
        <b>AdamW</b> and <b>Adam</b> optimizers for the GCN and normalizing flow 
        respectively. Key hyperparameters included:
      </p>
      <ul>
        <li>GCN: 2 layers √ó 64 hidden units, ReLU activations, global mean pooling</li>
        <li>Normalizing Flow: MAF with 5 coupling layers, 128 hidden units each</li>
        <li>Early stopping based on validation MSE</li>
      </ul>
    </section>

    <!-- Results -->
    <section>
    <h2>Results</h2>
    <ul>
        <li><b>Scratch GNN:</b> Successfully recovered parameters <i>r<sub>s</sub></i> and <i>œÅ<sub>0</sub></i>; less accurate on Œ≥ due to its role in the exponent.</li>
        <li><b>GNN + Flow:</b> Achieved posteriors consistent with published contours, reducing errors by ~2x compared to Jeans analysis.</li>
        <li><b>RMSE (10k test galaxies):</b> Œ≥ = 0.18 vs 0.35 (Jeans), log r<sub>s</sub> = 0.12 vs 0.22, log œÅ<sub>0</sub> = 0.15 vs 0.28.</li>
        <li>Credible intervals were well-calibrated, especially near the stellar half-light radius.</li>
    </ul>

    <h3>Scratch GNN Parameter Recovery</h3>
    <img src="../assets/figure1.png" alt="Scratch GNN parameter recovery" class="project-img">

    <h3>Normalizing Flow Posterior Inference</h3>
    <img src="../assets/figure2.png" alt="Normalizing Flow posterior inference" class="project-img">

    <h3>Jeans Analysis Comparison</h3>
    <img src="../assets/figure3.png" alt="Classical Jeans analysis comparison" class="project-img">

    <h3>Training vs Validation Loss</h3>
    <img src="../assets/loss_curve.png" alt="Training vs validation loss curve" class="project-img">
    </section>


    <!-- Impact -->
    <section>
      <h2>Impact</h2>
      <p>
        This project demonstrates that <b>GNN-based inference significantly 
        outperforms classical Jeans modeling</b> for recovering dark matter 
        density profiles. Beyond validation, our results highlight the 
        reproducibility challenges of deep learning in astrophysics, 
        emphasizing the influence of hyperparameters and model choices.
      </p>
    </section>

    <!-- Tech Stack -->
    <section>
      <h2>Tech Stack</h2>
      <p>
        Python ¬∑ PyTorch ¬∑ PyTorch Geometric ¬∑ StarSampler ¬∑ GNNs ¬∑ Normalizing Flows
      </p>
    </section>

    <!-- Links -->
    <section>
      <h2>Links</h2>
      <p>
        <a href="https://github.com/miruyoun/DS542_GCN_Model" target="_blank" class="resume-btn">
          üîó View GitHub Repository
        </a>
      </p>
      <p>
        <a href="../assets/DarkMatterGNN_Paper.pdf" target="_blank" class="resume-btn">
          üìÑ Read Full Paper (PDF)
        </a>
      </p>

      <p><a href="../index.html">‚Üê Back to Portfolio</a></p>
    </section>
  </main>

  <!-- Site Footer -->
  <footer>
    <p>¬© 2025 Mi-Ru Youn</p>
  </footer>
</body>
</html>
